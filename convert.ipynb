{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./miniaturautonomie_lanedetection/include/\")\n",
    "sys.path.append(\"./include/\")\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import random\n",
    "import render\n",
    "import cv2\n",
    "import albumentations as Alb\n",
    "from DataGenerator import DataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import coremltools as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 13:25:56.809481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:56.815511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:56.816056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:56.816927: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 13:25:56.817750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:56.818429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:56.818917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:57.113853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:57.114190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:57.114489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-10 13:25:57.114768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5303 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "output_dir = f\"./output/tiny_ft_g125_a7\"\n",
    "model_name = \"model\"\n",
    "model = load_model(f\"{output_dir}/{model_name}.h5\", custom_objects={'iou_score':None, \n",
    "                                                            'f1_score':None, \n",
    "                                                            'precision_m':None, \n",
    "                                                            'recall_m':None,\n",
    "                                                            'tversky_loss': None,\n",
    "                                                            'focal_tversky_loss': None,\n",
    "                                                            'weighted_ce': None,\n",
    "                                                            'dice_loss': None})\n",
    "input_img_size = (480, 640)\n",
    "output_width = 640 \n",
    "output_height = 224 \n",
    "input_width = 640 \n",
    "input_height = 224\n",
    "number_classes = 7 # outer, middle_curb, guide_lane, solid_lane, hold_line, zebra, background\n",
    "packages = ['knuff_main1', 'knuff_main2', 'knuff_main3', 'knuff_hill', 'highway', 'knuff_main5', 'knuff_main6']\n",
    "image_path = './data/'\n",
    "annotation_path = './annotation_v3.0/'\n",
    "\n",
    "# generate absolute list of all img and masks paths\n",
    "image_paths = []\n",
    "mask_paths = []\n",
    "# for package\n",
    "for index in range(len(packages)):\n",
    "    image_base_path = f\"{image_path}{packages[index]}/\"\n",
    "    masks_base_path = f\"{annotation_path}{packages[index]}/masks/\"\n",
    "\n",
    "    file_list = os.listdir(masks_base_path)\n",
    "    pattern = '*.png'\n",
    "    for filename in file_list:\n",
    "        if fnmatch.fnmatch(filename, pattern):\n",
    "            mask_paths.append(os.path.join(masks_base_path, filename))\n",
    "            image_name = filename[:len(filename)-3]+\"jpg\"\n",
    "            image_paths.append(os.path.join(image_base_path, image_name))\n",
    "            \n",
    "size_all = len(image_paths)\n",
    "random.Random(size_all).shuffle(image_paths)\n",
    "random.Random(size_all).shuffle(mask_paths)\n",
    "\n",
    "random.Random(size_all).shuffle(image_paths)\n",
    "random.Random(size_all).shuffle(mask_paths)\n",
    "\n",
    "random.Random(size_all).shuffle(image_paths)\n",
    "random.Random(size_all).shuffle(mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'architectures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3cebaac160dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# change image resolution if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmall_unet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msmall_unet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"res_conversion\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'architectures'"
     ]
    }
   ],
   "source": [
    "# change image resolution if needed\n",
    "import architectures.unet as unet\n",
    "import architectures.small_unet as small_unet\n",
    "\n",
    "new_model = unet.vgg(\"res_conversion\", 96, 224, number_classes, None)\n",
    "new_model.set_weights(model.get_weights())\n",
    "model.save(f\"{output_dir}/model_224_96.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLite with 8Bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rep = 100\n",
    "params = {\n",
    "    'batch_size': number_of_rep,\n",
    "    'input_img_size': input_img_size,\n",
    "    'target_img_size': (output_height, output_width),\n",
    "    'shuffle': True,\n",
    "    'n_channels': number_classes,\n",
    "    'augmentation': False\n",
    "}\n",
    "rep_gen = DataGenerator(image_paths, mask_paths, **params)\n",
    "representative_imgs = image_paths[:number_of_rep]\n",
    "representative_masks = mask_paths[:number_of_rep]\n",
    "\n",
    "imgs, masks = rep_gen.data_generation(representative_imgs, representative_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy8n6lsna/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpy8n6lsna/assets\n",
      "/home/danielriege/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2022-05-10 13:28:12.333455: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-05-10 13:28:12.333480: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-05-10 13:28:12.333663: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpy8n6lsna\n",
      "2022-05-10 13:28:12.360597: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-05-10 13:28:12.360629: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpy8n6lsna\n",
      "2022-05-10 13:28:12.448764: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-05-10 13:28:13.087033: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpy8n6lsna\n",
      "2022-05-10 13:28:13.296745: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 963082 microseconds.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EndVector() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m converter\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# converter.default_ranges_stats = (0,255)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tflite_quant_model \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tflite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite(tflite_quant_model)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:803\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(convert_func)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    802\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:789\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m    788\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[0;32m--> 789\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m elapsed_time_ms \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mprocess_time() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1210\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1199\u001b[0m   \u001b[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;124;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m   saved_model_convert_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1192\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m   graph_def, input_tensors, output_tensors \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1190\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_keras_to_saved_model(temp_dir))\n\u001b[1;32m   1191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1195\u001b[0m   shutil\u001b[38;5;241m.\u001b[39mrmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:1009\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m result \u001b[38;5;241m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1004\u001b[0m     input_data\u001b[38;5;241m=\u001b[39mgraph_def,\n\u001b[1;32m   1005\u001b[0m     input_tensors\u001b[38;5;241m=\u001b[39minput_tensors,\n\u001b[1;32m   1006\u001b[0m     output_tensors\u001b[38;5;241m=\u001b[39moutput_tensors,\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconverter_kwargs)\n\u001b[0;32m-> 1009\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_tflite_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_quant_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental_new_quantizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:216\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    215\u001b[0m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n\u001b[0;32m--> 216\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/convert_phase.py:206\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    205\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter_error\u001b[38;5;241m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/lite.py:759\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    754\u001b[0m   \u001b[38;5;66;03m# Skip updating model io types if MLIR quantizer already takes care of it\u001b[39;00m\n\u001b[1;32m    755\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (quant_mode\u001b[38;5;241m.\u001b[39mis_post_training_integer_quantize() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    756\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_new_quantizer \u001b[38;5;129;01mand\u001b[39;00m quant_io \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    757\u001b[0m           (m_in_type \u001b[38;5;129;01min\u001b[39;00m [_dtypes\u001b[38;5;241m.\u001b[39mint8, _dtypes\u001b[38;5;241m.\u001b[39muint8, _dtypes\u001b[38;5;241m.\u001b[39mfloat32]) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    758\u001b[0m           (m_out_type \u001b[38;5;129;01min\u001b[39;00m [_dtypes\u001b[38;5;241m.\u001b[39mint8, _dtypes\u001b[38;5;241m.\u001b[39muint8, _dtypes\u001b[38;5;241m.\u001b[39mfloat32])):\n\u001b[0;32m--> 759\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_modify_model_io_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_in_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_out_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparsify_model():\n\u001b[1;32m    762\u001b[0m   model \u001b[38;5;241m=\u001b[39m _mlir_sparsify(model)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/util.py:979\u001b[0m, in \u001b[0;36mmodify_model_io_type\u001b[0;34m(model, inference_input_type, inference_output_type)\u001b[0m\n\u001b[1;32m    975\u001b[0m _modify_model_output_type(model_object, inference_output_type)\n\u001b[1;32m    977\u001b[0m _remove_redundant_quantize_ops(model_object)\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_model_from_object_to_bytearray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_object\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/util.py:561\u001b[0m, in \u001b[0;36m_convert_model_from_object_to_bytearray\u001b[0;34m(model_object)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Initial size of the buffer, which will grow automatically if needed\u001b[39;00m\n\u001b[1;32m    560\u001b[0m builder \u001b[38;5;241m=\u001b[39m flatbuffers\u001b[38;5;241m.\u001b[39mBuilder(\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m--> 561\u001b[0m model_offset \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m builder\u001b[38;5;241m.\u001b[39mFinish(model_offset, file_identifier\u001b[38;5;241m=\u001b[39m_TFLITE_FILE_IDENTIFIER)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(builder\u001b[38;5;241m.\u001b[39mOutput())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/lane_detection-lywnBSQa/lib/python3.9/site-packages/tensorflow/lite/python/schema_py_generated.py:5890\u001b[0m, in \u001b[0;36mModelT.Pack\u001b[0;34m(self, builder)\u001b[0m\n\u001b[1;32m   5888\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperatorCodes))):\n\u001b[1;32m   5889\u001b[0m         builder\u001b[38;5;241m.\u001b[39mPrependUOffsetTRelative(operatorCodeslist[i])\n\u001b[0;32m-> 5890\u001b[0m     operatorCodes \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEndVector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperatorCodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubgraphs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5892\u001b[0m     subgraphslist \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: EndVector() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# zero_img = np.zeros((1,output_height, output_width,3), dtype=np.float32)\n",
    "# one_img = np.ones((1,output_height, output_width,3), dtype=np.float32)\n",
    "\n",
    "# imgs = np.concatenate((imgs, zero_img))\n",
    "# imgs = np.concatenate((imgs, one_img))\n",
    "\n",
    "def representative_dataset():\n",
    "    for img in imgs:\n",
    "        #img = cv2.resize(img, (224, 96))\n",
    "        data = np.expand_dims(img, axis=0)\n",
    "        yield [data.astype(np.float32)]\n",
    "        \n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8  # or tf.uint8\n",
    "converter.inference_output_type = tf.uint8  # or tf.uint8\n",
    "converter.experimental_new_quantizer = True\n",
    "# converter.default_ranges_stats = (0,255)\n",
    "tflite_quant_model = converter.convert()\n",
    "open(f\"{output_dir}/{model_name}.tflite\", \"wb\").write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interpreter\n",
    "interpreter = tf.lite.Interpreter(model_path=f\"{output_dir}/model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "imgs = imgs[:2]\n",
    "masks = masks[:2]\n",
    "\n",
    "# Check if the input type is quantized, then rescale input data to uint8\n",
    "outputs = np.zeros((imgs.shape[0], imgs.shape[1], imgs.shape[2], 7), dtype=np.uint8)\n",
    "lite_imgs = []\n",
    "for i, img in enumerate(imgs):\n",
    "    \n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    print(f\"{input_scale} : {input_zero_point}\")\n",
    "    lite_img = np.array(img / input_scale + input_zero_point, dtype=np.uint8)\n",
    "    lite_imgs.append(lite_img)\n",
    "    lite_img = np.expand_dims(lite_img, axis=0)\n",
    "    interpreter.set_tensor(input_details[\"index\"], lite_img)\n",
    "    interpreter.invoke()\n",
    "    outputs[i] = interpreter.get_tensor(output_details[\"index\"])\n",
    "        \n",
    "\n",
    "predictions = model.predict(imgs)\n",
    "thres_value = 0.6\n",
    "\n",
    "f, axs = plt.subplots(len(predictions), 4, figsize=(20,4))\n",
    "for i, prediction in enumerate(predictions):\n",
    "    # show input img\n",
    "    axs[i,0].imshow(imgs[i])\n",
    "    axs[i,0].title.set_text('Input')\n",
    "    \n",
    "    # show prediction\n",
    "    ## clip to 0 or 1 with thres\n",
    "    clipped_pred = np.where(prediction > thres_value, 1, 0)\n",
    "    rgb_pred = render.render_rgb(clipped_pred)\n",
    "    \n",
    "    axs[i,1].imshow(rgb_pred)\n",
    "    axs[i,1].title.set_text('float32')\n",
    "    \n",
    "    # show input img\n",
    "    axs[i,2].imshow(lite_imgs[i])\n",
    "    axs[i,2].title.set_text('Input uint8 quantized')\n",
    "    \n",
    "    ## clip to 0 or 1 with thres\n",
    "    clipped_pred = np.where(outputs[i] > thres_value, 1, 0)\n",
    "    rgb_pred = render.render_rgb(clipped_pred)\n",
    "    \n",
    "    axs[i,3].imshow(rgb_pred)\n",
    "    axs[i,3].title.set_text('uint8')\n",
    "    \n",
    "f.savefig('tflite_0.027.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel = ct.convert(model)\n",
    "mlmodel.save(f\"{output_dir}/{model_name}.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "mlmodel = ct.models.MLModel(f\"{output_dir}/{model_name}.mlmodel\")\n",
    "\n",
    "imgs = imgs[:2]\n",
    "masks = masks[:2]\n",
    "\n",
    "# Check if the input type is quantized, then rescale input data to uint8\n",
    "outputs = np.zeros((imgs.shape[0], imgs.shape[1], imgs.shape[2], 7), dtype=np.uint8)\n",
    "lite_imgs = []\n",
    "for i, img in enumerate(imgs):\n",
    "    \n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    print(f\"{input_scale} : {input_zero_point}\")\n",
    "    lite_img = np.array(img / input_scale + input_zero_point, dtype=np.uint8)\n",
    "    lite_imgs.append(lite_img)\n",
    "    lite_img = np.expand_dims(lite_img, axis=0)\n",
    "    interpreter.set_tensor(input_details[\"index\"], lite_img)\n",
    "    interpreter.invoke()\n",
    "    outputs[i] = interpreter.get_tensor(output_details[\"index\"])\n",
    "        \n",
    "\n",
    "predictions = model.predict(imgs)\n",
    "thres_value = 0.6\n",
    "\n",
    "f, axs = plt.subplots(len(predictions), 4, figsize=(20,4))\n",
    "for i, prediction in enumerate(predictions):\n",
    "    # show input img\n",
    "    axs[i,0].imshow(imgs[i])\n",
    "    axs[i,0].title.set_text('Input')\n",
    "    \n",
    "    # show prediction\n",
    "    ## clip to 0 or 1 with thres\n",
    "    clipped_pred = np.where(prediction > thres_value, 1, 0)\n",
    "    rgb_pred = render.render_rgb(clipped_pred)\n",
    "    \n",
    "    axs[i,1].imshow(rgb_pred)\n",
    "    axs[i,1].title.set_text('float32')\n",
    "    \n",
    "    # show input img\n",
    "    axs[i,2].imshow(lite_imgs[i])\n",
    "    axs[i,2].title.set_text('Input uint8 quantized')\n",
    "    \n",
    "    ## clip to 0 or 1 with thres\n",
    "    clipped_pred = np.where(outputs[i] > thres_value, 1, 0)\n",
    "    rgb_pred = render.render_rgb(clipped_pred)\n",
    "    \n",
    "    axs[i,3].imshow(rgb_pred)\n",
    "    axs[i,3].title.set_text('uint8')\n",
    "    \n",
    "f.savefig('tflite_0.027.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
